{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from spectrogram_encoder import SpectrogramEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR_PATH = 'voice-commands-classification-2025/train'\n",
    "TEST_DIR_PATH = 'voice-commands-classification-2025/adv_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "N_WORKERS = 6\n",
    "N_CLASSES = 35\n",
    "EPOCHS = 50\n",
    "LR = 0.01\n",
    "\n",
    "N_MFCC = 120\n",
    "NOISE_AMPLITUDE = 0.00\n",
    "MASK_PROB = 0.1\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda:0')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "def noise_waveform(waveform: torch.Tensor, noise_amplitude: float = 0.05) -> torch.Tensor:\n",
    "    noise = noise_amplitude * torch.randn(waveform.shape).to(waveform.device)\n",
    "    noisy_waveform = waveform + noise\n",
    "    noisy_waveform = torch.clamp(noisy_waveform, -1.0, 1.0)\n",
    "    return noisy_waveform\n",
    "\n",
    "class SpeechCommandDataset(Dataset):\n",
    "    def __init__(self, dir_path, data, labels=None, dict_label_to_index=None, transform=None, noise_amplitude=0.00):\n",
    "        self.dir_path = dir_path\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.dict_label_to_index = dict_label_to_index\n",
    "        self.transform = transform\n",
    "        self.noise_amplitude = noise_amplitude\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.data[idx]\n",
    "        waveform = np.load(os.path.join(self.dir_path, file_name))\n",
    "        if waveform.shape[1] < 16000:\n",
    "            waveform = np.pad(\n",
    "                waveform, pad_width=((0, 0), (0, 16000 - waveform.shape[1])),\n",
    "                mode='constant',\n",
    "                constant_values=0\n",
    "            )\n",
    "\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "        if self.noise_amplitude > 0:\n",
    "            waveform = noise_waveform(waveform, self.noise_amplitude)\n",
    "\n",
    "        if self.transform != None:\n",
    "            spectrogram = self.transform(waveform.float())\n",
    "        else:\n",
    "            spectrogram = None\n",
    "        \n",
    "        out_labels = []\n",
    "        if self.labels is not None:\n",
    "            if self.labels[idx] in self.dict_label_to_index:\n",
    "                out_labels = self.dict_label_to_index[self.labels[idx]]\n",
    "\n",
    "        return waveform, spectrogram, out_labels, int(file_name.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop': 0,\n",
       " 'go': 1,\n",
       " 'right': 2,\n",
       " 'dog': 3,\n",
       " 'left': 4,\n",
       " 'yes': 5,\n",
       " 'zero': 6,\n",
       " 'four': 7,\n",
       " 'bird': 8,\n",
       " 'cat': 9,\n",
       " 'five': 10,\n",
       " 'off': 11,\n",
       " 'learn': 12,\n",
       " 'six': 13,\n",
       " 'two': 14,\n",
       " 'on': 15,\n",
       " 'up': 16,\n",
       " 'three': 17,\n",
       " 'nine': 18,\n",
       " 'one': 19,\n",
       " 'follow': 20,\n",
       " 'wow': 21,\n",
       " 'seven': 22,\n",
       " 'sheila': 23,\n",
       " 'down': 24,\n",
       " 'no': 25,\n",
       " 'bed': 26,\n",
       " 'eight': 27,\n",
       " 'house': 28,\n",
       " 'tree': 29,\n",
       " 'visual': 30,\n",
       " 'forward': 31,\n",
       " 'marvin': 32,\n",
       " 'backward': 33,\n",
       " 'happy': 34}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\n",
    "    os.path.join(TRAIN_DIR_PATH, 'metadata.csv')\n",
    ")\n",
    "dict_label_to_index = {}\n",
    "dict_index_to_label = {}\n",
    "for index, key in enumerate(df_train['label'].unique()):\n",
    "    dict_label_to_index[key] = index\n",
    "    dict_index_to_label[index] = key\n",
    "\n",
    "dict_label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data, df_val_data = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_data = df_train_data.file_name.values\n",
    "train_labels = df_train_data.label.values\n",
    "\n",
    "val_data = df_val_data.file_name.values\n",
    "val_labels = df_val_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader, transform\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    SpeechCommandDataset(\n",
    "        dir_path=TRAIN_DIR_PATH,\n",
    "        data=train_data,\n",
    "        labels=train_labels,\n",
    "        dict_label_to_index=dict_label_to_index,\n",
    "        transform=torchaudio.transforms.MFCC(n_mfcc=N_MFCC, log_mels=True),\n",
    "        noise_amplitude=NOISE_AMPLITUDE\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=N_WORKERS\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    SpeechCommandDataset(\n",
    "        dir_path=TRAIN_DIR_PATH,\n",
    "        data=val_data,\n",
    "        labels=val_labels,\n",
    "        dict_label_to_index=dict_label_to_index,\n",
    "        transform=torchaudio.transforms.MFCC(n_mfcc=N_MFCC, log_mels=True),\n",
    "        noise_amplitude=0.0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=N_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 16000]) torch.Size([512, 1, 120, 81])\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataloader:\n",
    "    print(item[0].shape, item[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_class, hidden_dim=96, mask_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.sg_enc = SpectrogramEncoder(n_layer=4, n_head=6, hidden_dim=hidden_dim, mask_prob=mask_prob)\n",
    "        self.lm_head = nn.Linear(hidden_dim, n_class)\n",
    "\n",
    "    def forward(self, sg):\n",
    "        sg = self.sg_enc(sg)\n",
    "        sg = sg.transpose(-1, -2)\n",
    "        sg = F.avg_pool1d(sg, sg.shape[-1])\n",
    "        sg = sg.transpose(-1, -2)\n",
    "        sg = self.lm_head(sg)\n",
    "        return F.log_softmax(sg, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M5(hidden_dim=N_MFCC, n_class=35, mask_prob=MASK_PROB)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 35])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(4, 1, 16000)\n",
    "input_sp = torchaudio.transforms.MFCC(n_mfcc=N_MFCC, log_mels=True)(input_image).squeeze(1).transpose(-1, -2).to(DEVICE)\n",
    "model = model.to(DEVICE)\n",
    "result = model(input_sp)\n",
    "print(result.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step):\n",
    "    return max(0.0, float(EPOCHS - current_step) / EPOCHS)\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_data: DataLoader, valid_data: DataLoader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    accuracy_train = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=N_CLASSES).to(DEVICE)\n",
    "    accuracy_val = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=N_CLASSES).to(DEVICE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for x, x_sp, y, _ in train_data:\n",
    "            x = x.to(DEVICE)\n",
    "            x_sp = x_sp.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = model(x_sp.squeeze(1).transpose(-1, -2)).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            _, preds = torch.max(y_hat, 1)\n",
    "\n",
    "            accuracy_train(\n",
    "                y_hat,\n",
    "                y\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        for x, x_sp, y, _ in valid_data:\n",
    "            x = x.to(DEVICE)\n",
    "            x_sp = x_sp.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y_hat = model(x_sp.squeeze(1).transpose(-1, -2)).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            _, preds = torch.max(y_hat, 1)\n",
    "\n",
    "            accuracy_val(\n",
    "                y_hat,\n",
    "                y\n",
    "            )\n",
    "\n",
    "        train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        val_loss = val_loss / len(valid_dataloader.dataset)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {accuracy_train.compute():.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {accuracy_val.compute():.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 1.1797, Train Acc: 0.6603\n",
      "Val Loss: 0.4985, Val Acc: 0.8515\n",
      "Epoch 2/50\n",
      "Train Loss: 0.4507, Train Acc: 0.7630\n",
      "Val Loss: 0.3791, Val Acc: 0.8680\n",
      "Epoch 3/50\n",
      "Train Loss: 0.3559, Train Acc: 0.8065\n",
      "Val Loss: 0.3408, Val Acc: 0.8784\n",
      "Epoch 4/50\n",
      "Train Loss: 0.3022, Train Acc: 0.8320\n",
      "Val Loss: 0.2885, Val Acc: 0.8867\n",
      "Epoch 5/50\n",
      "Train Loss: 0.2778, Train Acc: 0.8488\n",
      "Val Loss: 0.2735, Val Acc: 0.8929\n",
      "Epoch 6/50\n",
      "Train Loss: 0.2570, Train Acc: 0.8609\n",
      "Val Loss: 0.2621, Val Acc: 0.8978\n",
      "Epoch 7/50\n",
      "Train Loss: 0.2364, Train Acc: 0.8706\n",
      "Val Loss: 0.2462, Val Acc: 0.9016\n",
      "Epoch 8/50\n",
      "Train Loss: 0.2265, Train Acc: 0.8781\n",
      "Val Loss: 0.2471, Val Acc: 0.9048\n",
      "Epoch 9/50\n",
      "Train Loss: 0.2163, Train Acc: 0.8843\n",
      "Val Loss: 0.2282, Val Acc: 0.9080\n",
      "Epoch 10/50\n",
      "Train Loss: 0.2089, Train Acc: 0.8894\n",
      "Val Loss: 0.2298, Val Acc: 0.9104\n",
      "Epoch 11/50\n",
      "Train Loss: 0.2001, Train Acc: 0.8939\n",
      "Val Loss: 0.2309, Val Acc: 0.9124\n",
      "Epoch 12/50\n",
      "Train Loss: 0.1861, Train Acc: 0.8979\n",
      "Val Loss: 0.2161, Val Acc: 0.9144\n",
      "Epoch 13/50\n",
      "Train Loss: 0.1861, Train Acc: 0.9012\n",
      "Val Loss: 0.2317, Val Acc: 0.9157\n",
      "Epoch 14/50\n",
      "Train Loss: 0.1789, Train Acc: 0.9043\n",
      "Val Loss: 0.2165, Val Acc: 0.9172\n",
      "Epoch 15/50\n",
      "Train Loss: 0.1729, Train Acc: 0.9071\n",
      "Val Loss: 0.2037, Val Acc: 0.9187\n",
      "Epoch 16/50\n",
      "Train Loss: 0.1702, Train Acc: 0.9097\n",
      "Val Loss: 0.2132, Val Acc: 0.9199\n",
      "Epoch 17/50\n",
      "Train Loss: 0.1604, Train Acc: 0.9120\n",
      "Val Loss: 0.2119, Val Acc: 0.9209\n",
      "Epoch 18/50\n",
      "Train Loss: 0.1570, Train Acc: 0.9142\n",
      "Val Loss: 0.2227, Val Acc: 0.9218\n",
      "Epoch 19/50\n",
      "Train Loss: 0.1522, Train Acc: 0.9162\n",
      "Val Loss: 0.2210, Val Acc: 0.9225\n",
      "Epoch 20/50\n",
      "Train Loss: 0.1472, Train Acc: 0.9181\n",
      "Val Loss: 0.1972, Val Acc: 0.9236\n",
      "Epoch 21/50\n",
      "Train Loss: 0.1385, Train Acc: 0.9199\n",
      "Val Loss: 0.1982, Val Acc: 0.9243\n",
      "Epoch 22/50\n",
      "Train Loss: 0.1368, Train Acc: 0.9216\n",
      "Val Loss: 0.1965, Val Acc: 0.9252\n",
      "Epoch 23/50\n",
      "Train Loss: 0.1313, Train Acc: 0.9232\n",
      "Val Loss: 0.2069, Val Acc: 0.9258\n",
      "Epoch 24/50\n",
      "Train Loss: 0.1299, Train Acc: 0.9247\n",
      "Val Loss: 0.1971, Val Acc: 0.9266\n",
      "Epoch 25/50\n",
      "Train Loss: 0.1270, Train Acc: 0.9261\n",
      "Val Loss: 0.2083, Val Acc: 0.9272\n",
      "Epoch 26/50\n",
      "Train Loss: 0.1233, Train Acc: 0.9274\n",
      "Val Loss: 0.1962, Val Acc: 0.9278\n",
      "Epoch 27/50\n",
      "Train Loss: 0.1127, Train Acc: 0.9288\n",
      "Val Loss: 0.1927, Val Acc: 0.9285\n",
      "Epoch 28/50\n",
      "Train Loss: 0.1113, Train Acc: 0.9301\n",
      "Val Loss: 0.1912, Val Acc: 0.9291\n",
      "Epoch 29/50\n",
      "Train Loss: 0.1064, Train Acc: 0.9314\n",
      "Val Loss: 0.2032, Val Acc: 0.9297\n",
      "Epoch 30/50\n",
      "Train Loss: 0.1040, Train Acc: 0.9326\n",
      "Val Loss: 0.1950, Val Acc: 0.9302\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0985, Train Acc: 0.9337\n",
      "Val Loss: 0.1989, Val Acc: 0.9307\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0941, Train Acc: 0.9349\n",
      "Val Loss: 0.1976, Val Acc: 0.9312\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0905, Train Acc: 0.9360\n",
      "Val Loss: 0.2005, Val Acc: 0.9316\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0869, Train Acc: 0.9371\n",
      "Val Loss: 0.1998, Val Acc: 0.9320\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0806, Train Acc: 0.9381\n",
      "Val Loss: 0.2055, Val Acc: 0.9324\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0784, Train Acc: 0.9392\n",
      "Val Loss: 0.1949, Val Acc: 0.9329\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0721, Train Acc: 0.9402\n",
      "Val Loss: 0.2058, Val Acc: 0.9333\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0667, Train Acc: 0.9412\n",
      "Val Loss: 0.2021, Val Acc: 0.9336\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0618, Train Acc: 0.9422\n",
      "Val Loss: 0.2075, Val Acc: 0.9340\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0562, Train Acc: 0.9433\n",
      "Val Loss: 0.2064, Val Acc: 0.9344\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0534, Train Acc: 0.9443\n",
      "Val Loss: 0.2168, Val Acc: 0.9347\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0513, Train Acc: 0.9452\n",
      "Val Loss: 0.2139, Val Acc: 0.9350\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0453, Train Acc: 0.9462\n",
      "Val Loss: 0.2103, Val Acc: 0.9354\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0418, Train Acc: 0.9471\n",
      "Val Loss: 0.2136, Val Acc: 0.9357\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0372, Train Acc: 0.9481\n",
      "Val Loss: 0.2142, Val Acc: 0.9361\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0340, Train Acc: 0.9490\n",
      "Val Loss: 0.2154, Val Acc: 0.9364\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0311, Train Acc: 0.9499\n",
      "Val Loss: 0.2175, Val Acc: 0.9367\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0286, Train Acc: 0.9508\n",
      "Val Loss: 0.2194, Val Acc: 0.9370\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0255, Train Acc: 0.9517\n",
      "Val Loss: 0.2193, Val Acc: 0.9373\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0237, Train Acc: 0.9525\n",
      "Val Loss: 0.2211, Val Acc: 0.9376\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_data=train_dataloader,\n",
    "    valid_data=valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    os.path.join(TEST_DIR_PATH, 'metadata.csv')\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SpeechCommandDataset(\n",
    "        dir_path=TEST_DIR_PATH,\n",
    "        data=df_test.file_name.values,\n",
    "        labels=None,\n",
    "        dict_label_to_index=dict_label_to_index,\n",
    "        transform=torchaudio.transforms.MFCC(n_mfcc=N_MFCC, log_mels=True),\n",
    "        noise_amplitude=0.0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=N_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE PREDICTIONS AND SUBMIT\n",
    "results = {\n",
    "    'id': [],\n",
    "    'label': []\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "for x, x_sp, y, ids in test_dataloader:\n",
    "    x = x.to(DEVICE)\n",
    "    x_sp = x_sp.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x_sp.squeeze(1).transpose(-1, -2)).squeeze()\n",
    "        _, preds = torch.max(y_hat, 1)\n",
    "        for i in range(len(preds)):\n",
    "            results[\"id\"].append(ids[i].item())\n",
    "            results[\"label\"].append(dict_index_to_label[int(preds[i].item())])\n",
    "        \n",
    "\n",
    "pd.DataFrame(results).to_csv(\n",
    "    'submission.csv',\n",
    "    columns=['id', 'label'],\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'spectrogram_encoder.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
