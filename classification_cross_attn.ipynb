{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from waveform_encoder import WaveformEncoder\n",
    "from spectrogram_encoder import SpectrogramEncoder\n",
    "from wf_sg_cross_attn import WaveformSpectrogramCrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR_PATH = 'voice-commands-classification-2025/train'\n",
    "TEST_DIR_PATH = 'voice-commands-classification-2025/adv_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "N_WORKERS = 6\n",
    "N_CLASSES = 35\n",
    "EPOCHS = 20\n",
    "LR = 0.005\n",
    "\n",
    "N_MFCC = 120\n",
    "NOISE_AMPLITUDE = 0.00\n",
    "MASK_PROB = 0.1\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda:0')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "def noise_waveform(waveform: torch.Tensor, noise_amplitude: float = 0.05) -> torch.Tensor:\n",
    "    noise = noise_amplitude * torch.randn(waveform.shape).to(waveform.device)\n",
    "    noisy_waveform = waveform + noise\n",
    "    noisy_waveform = torch.clamp(noisy_waveform, -1.0, 1.0)\n",
    "    return noisy_waveform\n",
    "\n",
    "class SpeechCommandDataset(Dataset):\n",
    "    def __init__(self, dir_path, data, labels=None, dict_label_to_index=None, transform=None, noise_amplitude=0.00):\n",
    "        self.dir_path = dir_path\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.dict_label_to_index = dict_label_to_index\n",
    "        self.transform = transform\n",
    "        self.noise_amplitude = noise_amplitude\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.data[idx]\n",
    "        waveform = np.load(os.path.join(self.dir_path, file_name))\n",
    "        if waveform.shape[1] < 16000:\n",
    "            waveform = np.pad(\n",
    "                waveform, pad_width=((0, 0), (0, 16000 - waveform.shape[1])),\n",
    "                mode='constant',\n",
    "                constant_values=0\n",
    "            )\n",
    "\n",
    "        waveform = torch.from_numpy(waveform).float()\n",
    "        if self.noise_amplitude > 0:\n",
    "            waveform = noise_waveform(waveform, self.noise_amplitude)\n",
    "\n",
    "        if self.transform != None:\n",
    "            spectrogram = self.transform(waveform)\n",
    "        else:\n",
    "            spectrogram = None\n",
    "        \n",
    "        out_labels = []\n",
    "        if self.labels is not None:\n",
    "            if self.labels[idx] in self.dict_label_to_index:\n",
    "                out_labels = self.dict_label_to_index[self.labels[idx]]\n",
    "\n",
    "        return waveform, spectrogram, out_labels, int(file_name.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop': 0,\n",
       " 'go': 1,\n",
       " 'right': 2,\n",
       " 'dog': 3,\n",
       " 'left': 4,\n",
       " 'yes': 5,\n",
       " 'zero': 6,\n",
       " 'four': 7,\n",
       " 'bird': 8,\n",
       " 'cat': 9,\n",
       " 'five': 10,\n",
       " 'off': 11,\n",
       " 'learn': 12,\n",
       " 'six': 13,\n",
       " 'two': 14,\n",
       " 'on': 15,\n",
       " 'up': 16,\n",
       " 'three': 17,\n",
       " 'nine': 18,\n",
       " 'one': 19,\n",
       " 'follow': 20,\n",
       " 'wow': 21,\n",
       " 'seven': 22,\n",
       " 'sheila': 23,\n",
       " 'down': 24,\n",
       " 'no': 25,\n",
       " 'bed': 26,\n",
       " 'eight': 27,\n",
       " 'house': 28,\n",
       " 'tree': 29,\n",
       " 'visual': 30,\n",
       " 'forward': 31,\n",
       " 'marvin': 32,\n",
       " 'backward': 33,\n",
       " 'happy': 34}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\n",
    "    os.path.join(TRAIN_DIR_PATH, 'metadata.csv')\n",
    ")\n",
    "dict_label_to_index = {}\n",
    "dict_index_to_label = {}\n",
    "for index, key in enumerate(df_train['label'].unique()):\n",
    "    dict_label_to_index[key] = index\n",
    "    dict_index_to_label[index] = key\n",
    "\n",
    "dict_label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data, df_val_data = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_data = df_train_data.file_name.values\n",
    "train_labels = df_train_data.label.values\n",
    "\n",
    "val_data = df_val_data.file_name.values\n",
    "val_labels = df_val_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader, transform\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    SpeechCommandDataset(\n",
    "        dir_path=TRAIN_DIR_PATH,\n",
    "        data=train_data,\n",
    "        labels=train_labels,\n",
    "        dict_label_to_index=dict_label_to_index,\n",
    "        transform=torchaudio.transforms.MFCC(n_mfcc=N_MFCC, log_mels=True),\n",
    "        noise_amplitude=NOISE_AMPLITUDE\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=N_WORKERS\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    SpeechCommandDataset(\n",
    "        dir_path=TRAIN_DIR_PATH,\n",
    "        data=val_data,\n",
    "        labels=val_labels,\n",
    "        dict_label_to_index=dict_label_to_index,\n",
    "        transform=torchaudio.transforms.MFCC(n_mfcc=N_MFCC, log_mels=True),\n",
    "        noise_amplitude=0.0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=N_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 16000]) torch.Size([256, 1, 120, 81])\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataloader:\n",
    "    print(item[0].shape, item[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load waveform encoder state dict\n",
    "state_dict = torch.load(\"waveform_encoder.pt\", weights_only=True)\n",
    "wf_enc_weight = {k[7:]: v for k, v in state_dict.items() if 'wf_enc' in k}\n",
    "wf_enc = WaveformEncoder(n_input=1, stride=160, kernel_size=400, n_channel=N_MFCC)\n",
    "wf_enc.load_state_dict(wf_enc_weight)\n",
    "\n",
    "# Load spectrogram encoder state dict\n",
    "state_dict = torch.load(\"spectrogram_encoder.pt\", weights_only=True)\n",
    "sg_enc_weight = {k[7:]: v for k, v in state_dict.items() if 'sg_enc' in k}\n",
    "sg_enc = SpectrogramEncoder(n_layer=4, n_head=6, hidden_dim=N_MFCC, mask_prob=MASK_PROB)\n",
    "sg_enc.load_state_dict(sg_enc_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_class,\n",
    "            n_layer,\n",
    "            n_head,\n",
    "            wf_enc:WaveformSpectrogramCrossEncoder,\n",
    "            sg_enc:SpectrogramEncoder,\n",
    "            hidden_dim: int = 96,\n",
    "            mask_prob: float = 0.1,\n",
    "            cross_attn_dropout: float = 0.1,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.wf_enc = wf_enc\n",
    "        self.sg_enc = sg_enc\n",
    "        self.cross_attn = WaveformSpectrogramCrossEncoder(n_layer, n_head, hidden_dim, dropout=cross_attn_dropout, mask_prob=mask_prob)\n",
    "        self.out = nn.Linear(hidden_dim, n_class)\n",
    "\n",
    "    def forward(self, x, sg):\n",
    "        x = self.wf_enc(x)\n",
    "        sg = self.sg_enc(sg)\n",
    "        logits = self.cross_attn(sg, x)\n",
    "        logits = logits.transpose(-1, -2)\n",
    "        logits = F.avg_pool1d(logits, logits.shape[-1])\n",
    "        logits = logits.transpose(-1, -2)\n",
    "        logits = self.out(logits)\n",
    "        return F.log_softmax(logits, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M5(n_class=N_CLASSES, n_layer=2, n_head=4, hidden_dim=N_MFCC, wf_enc=wf_enc, sg_enc=sg_enc, mask_prob=MASK_PROB)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 35])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(4, 1, 16000)\n",
    "input_sp = torchaudio.transforms.MFCC(n_mfcc=N_MFCC, log_mels=True)(input_image).squeeze(1).transpose(-1, -2)\n",
    "model = model.to(DEVICE)\n",
    "result = model(input_image.to(DEVICE), input_sp.to(DEVICE))\n",
    "print(result.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step):\n",
    "    return max(0.0, float(EPOCHS - current_step) / EPOCHS)\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_data: DataLoader, valid_data: DataLoader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    accuracy_train = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=N_CLASSES).to(DEVICE)\n",
    "    accuracy_val = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=N_CLASSES).to(DEVICE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for x, x_sp, y, _ in train_data:\n",
    "            x = x.to(DEVICE)\n",
    "            x_sp = x_sp.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = model(x, x_sp.squeeze(1).transpose(-1, -2)).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            _, preds = torch.max(y_hat, 1)\n",
    "\n",
    "            accuracy_train(\n",
    "                y_hat,\n",
    "                y\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        for x, x_sp, y, _ in valid_data:\n",
    "            x = x.to(DEVICE)\n",
    "            x_sp = x_sp.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y_hat = model(x, x_sp.squeeze(1).transpose(-1, -2)).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            _, preds = torch.max(y_hat, 1)\n",
    "\n",
    "            accuracy_val(\n",
    "                y_hat,\n",
    "                y\n",
    "            )\n",
    "\n",
    "        train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        val_loss = val_loss / len(valid_dataloader.dataset)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {accuracy_train.compute():.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {accuracy_val.compute():.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze encoder weights to train cross attention layer\n",
    "\n",
    "for param in model.wf_enc.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.sg_enc.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 0.0789, Train Acc: 0.9826\n",
      "Val Loss: 0.2712, Val Acc: 0.9419\n",
      "Epoch 2/20\n",
      "Train Loss: 0.0168, Train Acc: 0.9892\n",
      "Val Loss: 0.2837, Val Acc: 0.9418\n",
      "Epoch 3/20\n",
      "Train Loss: 0.0150, Train Acc: 0.9915\n",
      "Val Loss: 0.2736, Val Acc: 0.9424\n",
      "Epoch 4/20\n",
      "Train Loss: 0.0109, Train Acc: 0.9929\n",
      "Val Loss: 0.2804, Val Acc: 0.9429\n",
      "Epoch 5/20\n",
      "Train Loss: 0.0118, Train Acc: 0.9937\n",
      "Val Loss: 0.2703, Val Acc: 0.9433\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0097, Train Acc: 0.9944\n",
      "Val Loss: 0.2935, Val Acc: 0.9430\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0084, Train Acc: 0.9949\n",
      "Val Loss: 0.2698, Val Acc: 0.9436\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0066, Train Acc: 0.9954\n",
      "Val Loss: 0.2641, Val Acc: 0.9441\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0073, Train Acc: 0.9958\n",
      "Val Loss: 0.2640, Val Acc: 0.9445\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0077, Train Acc: 0.9960\n",
      "Val Loss: 0.2547, Val Acc: 0.9449\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0056, Train Acc: 0.9963\n",
      "Val Loss: 0.2576, Val Acc: 0.9452\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0053, Train Acc: 0.9965\n",
      "Val Loss: 0.2523, Val Acc: 0.9456\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0040, Train Acc: 0.9967\n",
      "Val Loss: 0.2565, Val Acc: 0.9459\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0039, Train Acc: 0.9969\n",
      "Val Loss: 0.2504, Val Acc: 0.9462\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0035, Train Acc: 0.9971\n",
      "Val Loss: 0.2496, Val Acc: 0.9465\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0030, Train Acc: 0.9972\n",
      "Val Loss: 0.2533, Val Acc: 0.9467\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0030, Train Acc: 0.9974\n",
      "Val Loss: 0.2497, Val Acc: 0.9469\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0027, Train Acc: 0.9975\n",
      "Val Loss: 0.2465, Val Acc: 0.9471\n",
      "Epoch 19/20\n",
      "Train Loss: 0.0025, Train Acc: 0.9976\n",
      "Val Loss: 0.2435, Val Acc: 0.9473\n",
      "Epoch 20/20\n",
      "Train Loss: 0.0021, Train Acc: 0.9977\n",
      "Val Loss: 0.2428, Val Acc: 0.9475\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_data=train_dataloader,\n",
    "    valid_data=valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cross_attn_encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('cross_attn_encoder.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step):\n",
    "    return max(0.0, float(EPOCHS - current_step) / EPOCHS)\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_data: DataLoader, valid_data: DataLoader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR // 10, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    accuracy_train = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=N_CLASSES).to(DEVICE)\n",
    "    accuracy_val = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=N_CLASSES).to(DEVICE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for x, x_sp, y, _ in train_data:\n",
    "            x = x.to(DEVICE)\n",
    "            x_sp = x_sp.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = model(x, x_sp.squeeze(1).transpose(-1, -2)).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            _, preds = torch.max(y_hat, 1)\n",
    "\n",
    "            accuracy_train(\n",
    "                y_hat,\n",
    "                y\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        for x, x_sp, y, _ in valid_data:\n",
    "            x = x.to(DEVICE)\n",
    "            x_sp = x_sp.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y_hat = model(x, x_sp.squeeze(1).transpose(-1, -2)).squeeze()\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "            _, preds = torch.max(y_hat, 1)\n",
    "\n",
    "            accuracy_val(\n",
    "                y_hat,\n",
    "                y\n",
    "            )\n",
    "\n",
    "        train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        val_loss = val_loss / len(valid_dataloader.dataset)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {accuracy_train.compute():.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {accuracy_val.compute():.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze encoder weights to fine-tune the model\n",
    "\n",
    "for param in model.wf_enc.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.sg_enc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 0.0021, Train Acc: 0.9999\n",
      "Val Loss: 0.2429, Val Acc: 0.9512\n",
      "Epoch 2/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9999\n",
      "Val Loss: 0.2429, Val Acc: 0.9512\n",
      "Epoch 3/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9999\n",
      "Val Loss: 0.2429, Val Acc: 0.9512\n",
      "Epoch 4/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2427, Val Acc: 0.9512\n",
      "Epoch 5/20\n",
      "Train Loss: 0.0021, Train Acc: 0.9999\n",
      "Val Loss: 0.2426, Val Acc: 0.9512\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9999\n",
      "Val Loss: 0.2429, Val Acc: 0.9512\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9999\n",
      "Val Loss: 0.2431, Val Acc: 0.9512\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9999\n",
      "Val Loss: 0.2431, Val Acc: 0.9512\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0022, Train Acc: 0.9998\n",
      "Val Loss: 0.2430, Val Acc: 0.9512\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2430, Val Acc: 0.9512\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2430, Val Acc: 0.9512\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0021, Train Acc: 0.9998\n",
      "Val Loss: 0.2431, Val Acc: 0.9512\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0019, Train Acc: 0.9998\n",
      "Val Loss: 0.2430, Val Acc: 0.9512\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2431, Val Acc: 0.9512\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2429, Val Acc: 0.9512\n",
      "Epoch 16/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2429, Val Acc: 0.9512\n",
      "Epoch 17/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2430, Val Acc: 0.9512\n",
      "Epoch 18/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2427, Val Acc: 0.9512\n",
      "Epoch 19/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2432, Val Acc: 0.9512\n",
      "Epoch 20/20\n",
      "Train Loss: 0.0020, Train Acc: 0.9998\n",
      "Val Loss: 0.2429, Val Acc: 0.9512\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_data=train_dataloader,\n",
    "    valid_data=valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    os.path.join(TEST_DIR_PATH, 'metadata.csv')\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SpeechCommandDataset(\n",
    "        dir_path=TEST_DIR_PATH,\n",
    "        data=df_test.file_name.values,\n",
    "        labels=None,\n",
    "        dict_label_to_index=dict_label_to_index,\n",
    "        transform=torchaudio.transforms.MFCC(n_mfcc=N_MFCC, log_mels=True),\n",
    "        noise_amplitude=0.0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=N_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE PREDICTIONS AND SUBMIT\n",
    "results = {\n",
    "    'id': [],\n",
    "    'label': []\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "for x, x_sp, y, ids in test_dataloader:\n",
    "    x = x.to(DEVICE)\n",
    "    x_sp = x_sp.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x, x_sp.squeeze(1).transpose(-1, -2)).squeeze()\n",
    "        _, preds = torch.max(y_hat, 1)\n",
    "        for i in range(len(preds)):\n",
    "            results[\"id\"].append(ids[i].item())\n",
    "            results[\"label\"].append(dict_index_to_label[int(preds[i].item())])\n",
    "        \n",
    "\n",
    "pd.DataFrame(results).to_csv(\n",
    "    'submission.csv',\n",
    "    columns=['id', 'label'],\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cross_attn_encoder_fine_tuned.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
